# ğŸš¢ User Survival Prediction (Titanic) â€“ Advanced MLOps Project

![MLOps](https://img.shields.io/badge/MLOps-CI%2FCD-blue)
![Python](https://img.shields.io/badge/Python-3.10%2B-brightgreen)
![License](https://img.shields.io/badge/License-GPL--3.0-orange)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![Airflow](https://img.shields.io/badge/Airflow-DAGs-informational)
![Prometheus](https://img.shields.io/badge/Prometheus-Monitoring-yellow)

An end-to-end MLOps project for predicting user survival on the Titanic dataset. This project demonstrates robust data pipelines, automated model training, CI/CD deployment with Docker and Airflow, monitoring with Prometheus, and a live web application for predictions. Built for reproducibility, scalability, and production-grade deployment.

> ğŸ“ **Repository**: `Advanced_Mlops_Project3_User_Survival_Predection`

---

## ğŸš€ Project Highlights

- ğŸš‚ **Data Pipeline**: Automated ingestion and processing of Titanic dataset
- ğŸ¤– **Model Training**: Feature engineering and model pipeline for survival prediction
- â˜ï¸ **MLOps Orchestration**: Airflow DAGs for workflow automation (ETL, training, serving)
- ğŸ“¦ **Dockerized Microservices**: Complete stack with Docker & Docker Compose
- ğŸ“Š **Monitoring**: Prometheus integration for model and app monitoring
- ğŸŒ **Web App**: Interactive UI for survival predictions

---

## ğŸ§  Technical Stack

### ğŸ› ï¸ Core Technologies
![Python](https://img.shields.io/badge/Python-3.10%2B-brightgreen)
![Docker](https://img.shields.io/badge/Docker-Ready-blue)
![Airflow](https://img.shields.io/badge/Airflow-DAGs-informational)
![Prometheus](https://img.shields.io/badge/Prometheus-Monitoring-yellow)
![Flask](https://img.shields.io/badge/Flask-WebApp-lightgrey)

### ğŸ“¦ Libraries & Utilities
- Pandas, NumPy, Scikit-learn (ML, Data)
- Airflow (Workflow orchestration)
- Prometheus (Monitoring)
- YAML (Config management)
- HTML/CSS (Web UI)

---

## ğŸ—ï¸ Project Structure

```bash
Advanced_Mlops_Project3_User_Survival_Predection/
â”œâ”€â”€ Dataset/
â”‚   â””â”€â”€ TITANIC/
â”‚       â””â”€â”€ Titanic-Dataset.csv     # Raw Titanic dataset
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database_config.py
â”‚   â”‚   â””â”€â”€ paths_config.py
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ exampledag.py
â”‚   â”‚   â””â”€â”€ extract_data_from_gcp.py
â”‚   â”œâ”€â”€ notebook/
â”‚   â”‚   â””â”€â”€ titanic.ipynb           # EDA & prototyping
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ training_pipeline.py    # End-to-end ML pipeline
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ custom_exception.py
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”‚   â”œâ”€â”€ feature_store.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ model_training.py
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â””â”€â”€ style.css               # Web app styling
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html              # Web app template
â”‚   â”œâ”€â”€ Dockerfile                  # Docker image build
â”‚   â”œâ”€â”€ docker-compose.yml          # Multi-service orchestration
â”‚   â”œâ”€â”€ prometheus.yml              # Monitoring config
â”‚   â”œâ”€â”€ application.py              # Flask app entry
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ setup.py
â”‚   â”œâ”€â”€ LICENSE
â”‚   â”œâ”€â”€ PDF and NOTES.pdf
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ STEPS NOTEPAD FILE              # Deployment & workflow notes
â”œâ”€â”€ .gitignore
```

---

## âš¡ Installation & Usage

### 1. Clone the repository
```bash
git clone https://github.com/mdzaheerjk/Advanced_Mlops_Project3_User_Survival_Predection.git
cd Advanced_Mlops_Project3_User_Survival_Predection/Code
```

### 2. Set up a virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Run the ML pipeline
```bash
python pipeline/training_pipeline.py
```

### 5. Run the web application
```bash
python application.py
```
Access the app at [http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## ğŸ³ Docker, Airflow & Monitoring

- **Docker:**  
  Use the provided `Dockerfile` and `docker-compose.yml` for local or cloud containerized deployment.

- **Airflow:**  
  DAG scripts in `dags/` automate data ingestion, preprocessing, and model training pipelines.

- **Prometheus:**  
  Monitoring is enabled by the `prometheus.yml` configuration.

---

## ğŸ“Š Example Use Cases

- **Survival Prediction**: Predict whether a Titanic passenger would survive based on input features
- **Web App Demo**: Interactive UI for submitting passenger details and viewing predictions
- **MLOps Pipeline Demo**: End-to-end automation from data to deployment

---

## âœï¸ Author

**Mohammed Zaheeruddin**  
ğŸ“ First-Year B.Tech Student | AI/ML & GenAI Enthusiast  
ğŸ« Shetty Institute of Technology, Gulbarga  
ğŸ“§ info.zaheerjk@gmail.com

---

## ğŸ“œ License

This project is licensed under the **GPL-3.0 License** â€“ see the LICENSE file for details.

---

#### Key Features:
1. Fully automated, modular MLOps pipeline for survival prediction
2. Dockerized microservices and orchestration with Docker Compose
3. Airflow DAGs for workflow automation
4. Real-time monitoring with Prometheus
5. User-friendly web interface for live model inference
6. Ready for education, research, or production adaptation
